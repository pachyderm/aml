{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Housing Prices Dataset\n",
        "\n",
        "The housing prices dataset used for this example is a reduced version of the original [Boston Housing Datset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html), which was originally collected by the U.S. Census Service. We choose to focus on three features of the originally dataset (RM, LSTST, and PTRATIO) and the output, or target (MEDV) that we are learning to predict.\n",
        "\n",
        "|Feature| Description|\n",
        "|---|---|\n",
        "|RM |       Average number of rooms per dwelling|\n",
        "|LSTAT |    A measurement of the socioeconomic status of people living in the area|\n",
        "|PTRATIO |  Pupil-teacher ratio by town - approximation of the local education system's quality|\n",
        "|MEDV |     Median value of owner-occupied homes in $1000's|\n",
        "\n",
        "Sample:\n",
        "\n",
        "|RM   |LSTAT|PTRATIO|MEDV|\n",
        "|-----|----|----|--------|\n",
        "|6.575|4.98|15.3|504000.0|\n",
        "|6.421|9.14|17.8|453600.0|\n",
        "|7.185|4.03|17.8|728700.0|\n",
        "|6.998|2.94|18.7|701400.0|\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before you can deploy this example you need to have the following components:\n",
        "\n",
        "1. AML-Pachyderm Integration following the setup from the root [README.md](../../README.MD) with the syncer mode set to `delimited`\n",
        "2. `pachctl` locally configured to connect to the Pachyderm instance in AML from the same setup instructions\n",
        "3. This notebook file and `utils.py` copied into your AzureML Workspace\n",
        "\n",
        "## Detailed Walkthrough\n",
        "\n",
        "### Step 1: Create an input data repository\n",
        "\n",
        "Once the Pachyderm cluster is running, create a data repository called `housing_data` where we will put our dataset.\n",
        "\n",
        "```bash\n",
        "$ pachctl create repo housing_data\n",
        "$ pachctl list repo\n",
        "NAME                CREATED             SIZE\n",
        "housing_data        3 seconds ago       0 B\n",
        "```\n",
        "\n",
        "### Step 2: Add the housing dataset to the repo\n",
        "\n",
        "Now we can add the data, which will kick off the processing automatically. If we update the data with a new commit, then the pipeline will automatically re-run. \n",
        "\n",
        "```bash\n",
        "$ pachctl put file housing_data@master:housing-simplified.csv -f data/housing-simplified-1.csv\n",
        "```\n",
        "\n",
        "We can inspect that the data is in the repository by looking at the files in the repository.\n",
        "\n",
        "```bash\n",
        "$ pachctl list file housing_data@master\n",
        "NAME                    TYPE SIZE\n",
        "/housing-simplified.csv file 12.14KiB\n",
        "```\n",
        "\n",
        "### Configure the Notebook\n",
        "\n",
        "Now Browse to the datasets on the left and you should see a `Pachyderm repo housing_data - delimited` dataset.\n",
        "\n",
        "1. Click on Consume\n",
        "2. Copy the sample usage and add it to this next code section and run it.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Paste Here"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628780830054
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Analysis\n",
        "\n",
        "When you run the code below, it creates a pair plot and a correlation matrix showing the relationship between features. By seeing what features are positively or negatively correlated to the target value (or each other), it can helps us understand what features may be valuable to the model.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from utils import data_analysis, set_dtypes\n",
        "\n",
        "data = dataset.to_pandas_dataframe()\n",
        "\n",
        "data = set_dtypes(data)\n",
        "\n",
        "data_analysis(data)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628780968613
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regresssion\n",
        "\n",
        "Setup regression\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from utils import load_data,train_model,test_model,create_learning_curve,plt\n",
        "\n",
        "input_data = data\n",
        "\n",
        "target_col = \"MEDV\"\n",
        "training_data, test_data = train_test_split(input_data, test_size=0.1,random_state=42)\n",
        "train_data, train_features, train_targets = load_data(training_data, target_col)\n",
        "print(\"Training set has {} data points with {} variables each.\".format(*train_data.shape))\n",
        "test_data, test_features, test_targets = load_data(test_data, target_col)\n",
        "print(\"Testing set has {} data points with {} variables each.\".format(*test_data.shape))\n",
        "\n",
        "reg = train_model(train_features, train_targets)\n",
        "test_results = test_model(reg, test_features, test_targets)\n",
        "create_learning_curve(reg, train_features, train_targets)\n",
        "plt.show()\n",
        "\n",
        "print(test_results)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628780106517
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we inspect the learning curve, we can see that there is a large gap between the training score and the validation score. This typically indicates that our model could benefit from the addition of more data. \n",
        "\n",
        "Now let's update our dataset with additional examples.\n",
        "\n",
        "### Step 6: Update Dataset\n",
        "Similar to the original housing prices example, we'll now add some new data. \n",
        "\n",
        "```bash\n",
        "$ pachctl put file housing_data@master:housing-simplified.csv -f data/housing-simplified-2.csv --overwrite\n",
        "```\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dataset = Dataset.get_by_name(workspace, name='Pachyderm repo housing_data - delimited', version='2')\n",
        "data2 = dataset.to_pandas_dataframe()\n",
        "\n",
        "data2 = set_dtypes(data2)\n",
        "\n",
        "data_analysis(data2)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628780992216
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Lets Rerun the Model with the new data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from utils import load_data,train_model,test_model,create_learning_curve,plt\n",
        "\n",
        "input_data = data2\n",
        "\n",
        "target_col = \"MEDV\"\n",
        "training_data, test_data = train_test_split(input_data, test_size=0.1,random_state=42)\n",
        "train_data, train_features, train_targets = load_data(training_data, target_col)\n",
        "print(\"Training set has {} data points with {} variables each.\".format(*train_data.shape))\n",
        "test_data, test_features, test_targets = load_data(test_data, target_col)\n",
        "print(\"Testing set has {} data points with {} variables each.\".format(*test_data.shape))\n",
        "\n",
        "reg = train_model(train_features, train_targets)\n",
        "test_results = test_model(reg, test_features, test_targets)\n",
        "create_learning_curve(reg, train_features, train_targets)\n",
        "plt.show()\n",
        "\n",
        "print(test_results)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628781006351
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow, now things look pretty good. lets try and add more data.\n",
        "\n",
        "### Step 7: Update Dataset Again\n",
        "Similar to the original housing prices example, we'll now add some new data. \n",
        "\n",
        "```bash\n",
        "$ pachctl put file housing_data@master:housing-simplified.csv -f data/housing-simplified-error.csv --overwrite\n",
        "```\n",
        "\n",
        "Now lets retrain the model with the new data.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dataset = Dataset.get_by_name(workspace, name='Pachyderm repo housing_data - delimited')\n",
        "data3 = dataset.to_pandas_dataframe()\n",
        "\n",
        "data3 = set_dtypes(data3)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from utils import load_data,train_model,test_model,create_learning_curve,plt\n",
        "\n",
        "input_data = data3\n",
        "\n",
        "target_col = \"MEDV\"\n",
        "training_data, test_data = train_test_split(input_data, test_size=0.1,random_state=42)\n",
        "train_data, train_features, train_targets = load_data(training_data, target_col)\n",
        "print(\"Training set has {} data points with {} variables each.\".format(*train_data.shape))\n",
        "test_data, test_features, test_targets = load_data(test_data, target_col)\n",
        "print(\"Testing set has {} data points with {} variables each.\".format(*test_data.shape))\n",
        "\n",
        "reg = train_model(train_features, train_targets)\n",
        "test_results = test_model(reg, test_features, test_targets)\n",
        "create_learning_curve(reg, train_features, train_targets)\n",
        "plt.show()\n",
        "\n",
        "print(test_results)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628781153370
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uh Oh, that doesn't look right. Lets look at the data:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data_analysis(data3)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628781161522
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yup, it looks like a bad test feature got in there. Let's switch back to the previous version by setting `version='2'`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dataset = Dataset.get_by_name(workspace, name='Pachyderm repo housing_data - delimited', version='2')\n",
        "data4 = dataset.to_pandas_dataframe()\n",
        "data4 = set_dtypes(data4)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from utils import load_data,train_model,test_model,create_learning_curve,plt\n",
        "\n",
        "input_data = data4\n",
        "\n",
        "target_col = \"MEDV\"\n",
        "training_data, test_data = train_test_split(input_data, test_size=0.1,random_state=42)\n",
        "train_data, train_features, train_targets = load_data(training_data, target_col)\n",
        "print(\"Training set has {} data points with {} variables each.\".format(*train_data.shape))\n",
        "test_data, test_features, test_targets = load_data(test_data, target_col)\n",
        "print(\"Testing set has {} data points with {} variables each.\".format(*test_data.shape))\n",
        "\n",
        "reg = train_model(train_features, train_targets)\n",
        "test_results = test_model(reg, test_features, test_targets)\n",
        "create_learning_curve(reg, train_features, train_targets)\n",
        "plt.show()\n",
        "\n",
        "print(test_results)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1628781373419
        }
      }
    }
  ],
  "metadata": {
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}